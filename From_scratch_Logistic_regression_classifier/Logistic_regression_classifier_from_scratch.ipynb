{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQBp6gtwOgeD"
   },
   "source": [
    "Logistic Regression Machine Learning Classification Algorithm Implementation.\n",
    "\n",
    "(By : Harshith Shankar Tarikere Ravikumar (19230323) and Shyam Kumar Sodankoor(19230735)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XyaweVhAOJsK"
   },
   "outputs": [],
   "source": [
    "import pandas as ps\n",
    "ps.options.mode.chained_assignment = None\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oL5YvI_WoBg-"
   },
   "outputs": [],
   "source": [
    "class preprocessing:\n",
    "    def __init__(self):\n",
    "        pass   \n",
    "    \n",
    "    '''\n",
    "    Author: Harshith Shankar Tarikere Ravikumar.\n",
    "    This function takes the inputs data frame and the train size and returns two data frames X_train and X_test based on the train_size.'''\n",
    "    def shuffle_split_data(self, X, train_size):\n",
    "        arr_rand = np.random.rand(X.shape[0])\n",
    "        split = arr_rand < np.percentile(arr_rand, train_size * 100)\n",
    "\n",
    "        X_train = X[split]\n",
    "        X_test =  X[~split]\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    ''' Author: Shyam Kumar Sodankoor.\n",
    "    This function takes a dataframe shuffle as input and normalizes it based on the max and min values of the whole dataframe and returns the normalized data frame.'''\n",
    "    def normalize_df(self, df):\n",
    "        df_norm = (df - df.min()) / (df.max() - df.min())\n",
    "        return df_norm\n",
    "\n",
    "    ''' Author: Harshith Shankar Tarikere Ravikumar.\n",
    "     This function takes the inputs file path, column names as a list, separator, starting index of the features, train data size and transpose required \n",
    "     and returns the training and testing data. If the user wants to input the train and test data separately. This can be achieved by setting \n",
    "     the train data size as 1.0 and 0 respectively and use the train_data only in first case and test_data only in the test data case. \n",
    "     In addition, the file is expected to be in the following format: Set of columns of non-required data (like sample_id) followed by set of features, followed by the labels. \n",
    "     It can be in the transposed format as well. '''\n",
    "    def split_data(self, filePath, column_names, seperator , feature_starting_index, train_data_size=0.67, transpose_required=True):\n",
    "        df = ps.read_csv(filePath, sep=seperator, header = None)\n",
    "        if transpose_required:\n",
    "            df = df.T\n",
    "            \n",
    "        df.columns = column_names\n",
    "\n",
    "        data = df.iloc[:,feature_starting_index:]\n",
    "\n",
    "        data.iloc[:, 0:-1] = np.float_(data.iloc[:, 0:-1])\n",
    "        data.iloc[:, 0:-1] = self.normalize_df(data.iloc[:, 0:-1])\n",
    "\n",
    "        data_train, data_test = self.shuffle_split_data(data, train_data_size)\n",
    "\n",
    "        return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NKVReniKeQzx"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    '''Author: Harshith Shankar Tarikere Ravikumar.'''\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "    \n",
    "    ''' Author: Harshith Shankar Tarikere Ravikumar.\n",
    "    This function adds the X-intercept or the bias for the given input and returns the input+intercept. '''\n",
    "    def add_theta0(self, X):\n",
    "        theta0 = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((theta0, X), axis=1)\n",
    "    \n",
    "    ''' Author: Harshith Shankar Tarikere Ravikumar.\n",
    "    This function calculates and returns the sigmoid for the given input. '''\n",
    "    def sigmoid_function(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    ''' Author: Harshith Shankar Tarikere Ravikumar.\n",
    "    This function does the preprocessing specific for the model. The input to this function is the dataframe, which consists of only the features and the labels data, \n",
    "    and the label names. It creates a dictionary with keys as tuples of different combinations of the classes and values as the data(features+labels) corresponding to \n",
    "    the labels in the keys. It also converts one of the labels to 0 and other to 1 for each item in the dictionary.'''\n",
    "    def data_processing(self, data, label_name):\n",
    "      import itertools\n",
    "      self.label_name = label_name\n",
    "      self.labels = data[label_name]\n",
    "      self.unique_labels = list(set(self.labels))\n",
    "      self.classes_list = list(itertools.combinations(self.unique_labels, 2))\n",
    "      self.classes_dict = {}\n",
    "      for classes in self.classes_list:\n",
    "          self.classes_dict[classes] = []\n",
    "      \n",
    "      for binclass,vectors in self.classes_dict.items():\n",
    "          self.classes_dict[binclass] = data[(data[self.label_name] == binclass[0]) | (data[self.label_name] == binclass[1])]\n",
    "          self.classes_dict[binclass][self.label_name][self.classes_dict[binclass][self.label_name] == binclass[0]] = 0 \n",
    "          self.classes_dict[binclass][self.label_name][self.classes_dict[binclass][self.label_name] == binclass[1]] = 1\n",
    "\n",
    "    ''' Author: Harshith Shankar Tarikere Ravikumar.\n",
    "    The set of features(X) and the corresponding outputs(y) are taken for a set of two classes. \n",
    "    X-intercept or the bias is added to this. ϴ is initially set to 0(An array based on the column size of X). \n",
    "    Then we go through a loop where we calculate the predicted output(h) based on the sigmoid of given features (X0 + ϴ1X1 + ϴ2X2 + ϴ3X3……… ϴnXn). \n",
    "    We calculate the gradient and subtract this from ϴ. This loop is run based on the input(num_iter), which finally gives the optimum ϴ.  \n",
    "\n",
    "    Whole of the above process is repeated for all the combinations of the classes and stored in a dictionary with keys as tuples of different combinations of the classes \n",
    "    and values as the ϴ values of their corresponding computed ϴ values. '''\n",
    "    def fit(self):\n",
    "       self.theta_dict = {}\n",
    "       for classes in self.classes_list:\n",
    "            self.theta_dict[classes] = []\n",
    "\n",
    "       for binclass,vectors in self.theta_dict.items():\n",
    "            X = self.classes_dict[binclass].iloc[:,0:-1]\n",
    "            y = self.classes_dict[binclass][self.label_name] \n",
    "\n",
    "            if self.fit_intercept:\n",
    "                X = self.add_theta0(X)\n",
    "        \n",
    "            # weights initialization\n",
    "            self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "            for i in range(self.num_iter):\n",
    "                z = np.array(np.dot(X, self.theta),dtype=np.float32)\n",
    "                #print(z)\n",
    "                h = self.sigmoid_function(z)\n",
    "                gradient = np.dot(X.T, (h - y)) / y.size\n",
    "                self.theta = self.theta - self.lr * gradient\n",
    "            \n",
    "            self.theta_dict[binclass] = self.theta\n",
    "    \n",
    "    ''' Author: Shyam Kumar Sodankoor.\n",
    "      This function takes as input set of features whose label has to be predicted. First, we add the X-intercept to this. \n",
    "      Then we compute the dot product of X with the calculated ϴ values and send this to the sigmoid function which returns the predicted class. \n",
    "      We do the above step for all the different combinations of classes and store the results in another dictionary(preds_dict)\n",
    "      with keys as tuples of different combinations of the classes and values as list of predictions for given test samples. \n",
    "      Now we have different predicted values for different combinations of classes. We use one vs one approach to get the exact prediction for each of the test data. '''\n",
    "    def predict(self, X):\n",
    "        self.preds_dict = {}\n",
    "        self.new_dict = {}\n",
    "        for classes in self.classes_list:\n",
    "            self.preds_dict[classes] = []\n",
    "            self.new_dict[classes] = []\n",
    "\n",
    "        if self.fit_intercept:\n",
    "                X = self.add_theta0(X)\n",
    "\n",
    "        for binclass,vectors in self.theta_dict.items():\n",
    "            preds_prob = self.sigmoid_function(np.array(np.dot(X, vectors),dtype=np.float32))\n",
    "            preds = preds_prob.round()\n",
    "            self.preds_dict[binclass] = preds\n",
    "            self.new_dict[binclass] = preds_prob\n",
    "        \n",
    "        predictions = self.oneVsone()\n",
    "        return predictions\n",
    "    \n",
    "    ''' Author: Shyam Kumar Sodankoor.\n",
    "    This function basically checks all the values of the preds_dict and returns the class which was predicted maximum number of times for a particular sample(voting). \n",
    "    There is also additional logic in case two or more classes have the same number of votes, where we check the probability result for the combinations of the classes \n",
    "    having same votes and we result the class having maximum probability. '''\n",
    "    def oneVsone(self):\n",
    "        import operator\n",
    "        import itertools\n",
    "        outputArray = []\n",
    "        for binclass,vectors in self.preds_dict.items():\n",
    "            vectors1 = [binclass[0] if item == 0.0 else binclass[1] for item in vectors]\n",
    "            outputArray.append(vectors1)\n",
    "\n",
    "        outputDict = {}\n",
    "        k=[]\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(0,len(outputArray[0])):\n",
    "            for classes in self.unique_labels:\n",
    "                outputDict[classes] = 0\n",
    "            for j in range(0, len(self.classes_list)):\n",
    "                outputDict[outputArray[j][i]] = outputDict[outputArray[j][i]] + 1\n",
    "    \n",
    "            max_vote = max(outputDict.values())\n",
    "\n",
    "            if max_vote <= len(self.unique_labels)/2:\n",
    "                k = [k for k,v in outputDict.items() if v==max_vote]\n",
    "                classes_list1 = list(itertools.combinations(k, 2))\n",
    "                minimum_diff = 0\n",
    "                minimum = 1\n",
    "                class_tuple = ()\n",
    "                for tuples in classes_list1:\n",
    "                    if self.new_dict[tuples][i] > 0.5:\n",
    "                        minimum_diff = 1 - self.new_dict[tuples][i]\n",
    "                    else:\n",
    "                        minimum_diff = self.new_dict[tuples][i]\n",
    "                if minimum_diff<minimum:\n",
    "                    minimum = minimum_diff\n",
    "                    class_tuple = tuples\n",
    "                if self.new_dict[class_tuple][i]>0.5:\n",
    "                    pred_class = class_tuple[1]\n",
    "                else:\n",
    "                    pred_class = class_tuple[0]\n",
    "            else:\n",
    "                pred_class = max(outputDict.items(), key=operator.itemgetter(1))[0]\n",
    "    \n",
    "            pred_class = max(outputDict.items(), key=operator.itemgetter(1))[0]\n",
    "            predictions.append(pred_class)\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    ''' Author: Shyam Kumar Sodankoor.\n",
    "    Takes inputs the predicted labels(returned from predict) and the actual labels(test data) and returns the percentage of labels properly classified in the test data. '''\n",
    "    def accuracy(self, predicted_labels, actual_labels):\n",
    "        return (predicted_labels == actual_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPRwFCCReQz0"
   },
   "outputs": [],
   "source": [
    "'''creating Logistic Regression object called 'model' with lr = 0.1, num_iter = 300 '''\n",
    "model = LogisticRegression(lr=0.1, num_iter=300)\n",
    "\n",
    "'''creating preprocessing object called 'prepro' '''\n",
    "prepro = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "-V7payZZr0Pq",
    "outputId": "17decb40-cce8-4a9f-9eea-2708e80fa508"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/hazelnuts.txt' does not exist: b'/hazelnuts.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-aef6b20b5de4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m       \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"length\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"width\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"thickness\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"surface_area\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"mass\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compactness\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"hardness\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shell_top_radius\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"water_content\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"carbohydrate_content\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"variety\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m       \u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"/hazelnuts.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseperator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mfeature_starting_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.67\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_required\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m       \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m       \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-de701660a512>\u001b[0m in \u001b[0;36msplit_data\u001b[1;34m(self, filePath, column_names, seperator, feature_starting_index, train_data_size, transpose_required)\u001b[0m\n\u001b[0;32m     28\u001b[0m      It can be in the transposed format as well. '''\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilePath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseperator\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mfeature_starting_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.67\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_required\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseperator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtranspose_required\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/hazelnuts.txt' does not exist: b'/hazelnuts.txt'"
     ]
    }
   ],
   "source": [
    "model_prediction = []\n",
    "'''\n",
    "Training and predicting an accuracy for 10 different random divisions and taking average on those 10 predicted accuracy.\n",
    "'''\n",
    "for i in range(0,10):\n",
    "      columns = [\"sample_id\", \"length\", \"width\", \"thickness\", \"surface_area\", \"mass\", \"compactness\", \"hardness\", \"shell_top_radius\", \"water_content\", \"carbohydrate_content\", \"variety\"]\n",
    "      data_train, data_test = prepro.split_data(filePath=\"/ hazelnuts.txt\", column_names=columns,seperator=\"\\t\" ,feature_starting_index=1, train_data_size=0.67, transpose_required=True)\n",
    "      X_train = data_train.iloc[:, 0:-1]\n",
    "      X_test = data_test.iloc[:, 0:-1]\n",
    "      y_train = data_train.iloc[:,-1]\n",
    "      y_test = data_test.iloc[:,-1]\n",
    "\n",
    "      model.data_processing(data_train, label_name = \"variety\")\n",
    "      model.fit()\n",
    "      preds_dict = model.predict(X_test)\n",
    "      model_prediction.append(model.accuracy(preds_dict,y_test))\n",
    "      \n",
    "print(\"Logistic Regression repeated with 10 different random divisions of data : \",model_prediction)\n",
    "print(\"Logistic Regression with an average accuracy  : \",np.mean(model_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdqZY2kXVnoq"
   },
   "outputs": [],
   "source": [
    "# confustion matrix\n",
    "from  sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "3jKOHpYGV2Co",
    "outputId": "995e8fca-f3b8-4d22-9ea5-bb3ba5b291de"
   },
   "outputs": [],
   "source": [
    "# printing classification report\n",
    "print(classification_report(y_test, preds_dict))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Logistic_regression_classifier_from_scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
